# app.py
import streamlit as st
import requests
from bs4 import BeautifulSoup
from pytube import YouTube
import subprocess
import os
import tempfile
from pydub import AudioSegment
import cv2
import pytesseract
import openai
from pathlib import Path
import math
import time

# ---------- ì„¤ì • ----------
st.set_page_config(page_title="ì˜ìƒ(ìŒì„±+í™”ë©´) ìë™ ìš”ì•½ê¸°", layout="wide")
openai.api_key = st.secrets["OPENAI_API_KEY"]

# ---------- ìœ í‹¸: ì‚¬ì´íŠ¸ì—ì„œ YouTube ë§í¬ ì¶”ì¶œ ----------
def extract_video_links_from_page(url):
    try:
        res = requests.get(url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
        res.raise_for_status()
    except Exception as e:
        st.error(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []
    soup = BeautifulSoup(res.text, "html.parser")
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "youtube.com/watch" in href or "youtu.be/" in href:
            # ì ˆëŒ€ URLì´ ì•„ë‹Œ ê²½ìš° ë„ë©”ì¸ ë¶™ì¼ í•„ìš”ëŠ” ìˆì§€ë§Œ ëŒ€ë¶€ë¶„ ì ˆëŒ€ URL
            if href.startswith("//"):
                href = "https:" + href
            if href.startswith("/"):
                href = requests.compat.urljoin(url, href)
            links.append(href)
    # ì¤‘ë³µ ì œê±°
    return list(dict.fromkeys(links))

# ---------- ìœ í‹¸: YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ (mp4) ----------
def download_youtube_video(video_url, target_dir):
    yt = YouTube(video_url)
    # ìµœì ì˜ progressive (audio+video) ìŠ¤íŠ¸ë¦¼
    stream = yt.streams.filter(file_extension="mp4", progressive=True, res="720p").first()
    if not stream:
        stream = yt.streams.filter(file_extension="mp4", progressive=True).order_by('resolution').desc().first()
    out_path = stream.download(output_path=target_dir)
    return out_path

# ---------- ì˜¤ë””ì˜¤ ì¶”ì¶œ using ffmpeg ----------
def extract_audio_with_ffmpeg(video_path, out_audio_path):
    cmd = [
        "ffmpeg", "-y", "-i", str(video_path),
        "-vn", "-acodec", "mp3", "-ar", "16000", "-ac", "1",
        str(out_audio_path)
    ]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
    return out_audio_path

# ---------- ì˜¤ë””ì˜¤ ë¶„í•  (pydub) ----------
def split_audio_to_chunks(audio_path, chunk_minutes=3):
    audio = AudioSegment.from_file(audio_path)
    chunk_ms = chunk_minutes * 60 * 1000
    chunks = []
    total_ms = len(audio)
    for i in range(0, total_ms, chunk_ms):
        chunk = audio[i: i + chunk_ms]
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        chunk.export(tmp.name, format="mp3")
        chunks.append(tmp.name)
    return chunks

# ---------- Whisperë¡œ ì „ì‚¬ (OpenAI) ----------
def transcribe_audio_file(path):
    # openai ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ audio transcription í˜¸ì¶œ
    with open(path, "rb") as f:
        # ëª¨ë¸ ì´ë¦„ì€ "whisper-1"
        res = openai.Audio.transcribe(model="whisper-1", file=f)
    # resëŠ” dict í˜•íƒœë¡œ 'text' í¬í•¨
    return res.get("text", "")

def transcribe_audio_chunks(chunk_paths, progress_callback=None):
    texts = []
    for i, p in enumerate(chunk_paths):
        if progress_callback:
            progress_callback(i, len(chunk_paths), f"Transcribing chunk {i+1}/{len(chunk_paths)}")
        try:
            txt = transcribe_audio_file(p)
        except Exception as e:
            txt = f"[Transcription failed chunk {i+1}: {e}]"
        texts.append(txt)
    return "\n".join(texts)

# ---------- OCR: í”„ë ˆì„ ì¶”ì¶œ + pytesseract ----------
def ocr_extract_text_from_video(video_path, frame_interval_seconds=2, lang="kor+eng", max_frames=200):
    cap = cv2.VideoCapture(str(video_path))
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    frame_interval = int(fps * frame_interval_seconds)
    texts = []
    frame_idx = 0
    saved_frames = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % frame_interval == 0:
            # ì „ì²˜ë¦¬: ê·¸ë ˆì´ìŠ¤ì¼€ì¼ -> ì„ê³„ì¹˜(ì„ íƒì )
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # pytesseractë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                txt = pytesseract.image_to_string(gray, lang=lang)
                if txt and txt.strip():
                    texts.append(txt.strip())
            except Exception:
                # pytesseract ì„¤ì • ë¬¸ì œ ë“±
                pass
            saved_frames += 1
            if saved_frames >= max_frames:
                break
        frame_idx += 1
    cap.release()
    return "\n".join(texts)

# ---------- GPT ìš”ì•½ (ì˜¤ë””ì˜¤ ì „ì‚¬ + OCR í•©ì³ì„œ) ----------
def summarize_combined_text(transcript_text, ocr_text, model="gpt-4o-mini", max_tokens=400):
    system = "ë‹¹ì‹ ì€ ì˜ìƒ ë‚´ìš©(ìŒì„± ì „ì‚¬ + í™”ë©´ OCR í…ìŠ¤íŠ¸)ì„ ì „ë¬¸ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."
    user_in = (
        "ì•„ë˜ ë‘ ì¢…ë¥˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•´ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ëŠ” ìš”ì•½ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”:\n"
        "1) í•œ ë¬¸ì¥ ìš”ì•½(í•µì‹¬) 2) í•µì‹¬ í¬ì¸íŠ¸(ë¶ˆë¦¿ 5ê°œ ì´ë‚´) 3) ì˜ìƒ ì‹œê°„ìˆœ ìš”ì•½(ê°€ëŠ¥í•˜ë©´ íƒ€ì„ë¼ì¸) "
        "4) ì¤‘ìš” í‚¤ì›Œë“œ(íƒœê·¸ í˜•ì‹)\n\n"
        "==== ì˜¤ë””ì˜¤ ì „ì‚¬ ====\n"
        f"{transcript_text[:30000]}\n"  # ê¸¸ì´ ì œí•œ: ì˜ë¼ì„œ ë³´ëƒ„
        "\n==== í™”ë©´ OCR í…ìŠ¤íŠ¸ ====\n"
        f"{ocr_text[:10000]}"
    )
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user_in}
    ]
    resp = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=0.2
    )
    return resp.choices[0].message["content"]

# ---------- Streamlit UI ----------
st.title("ğŸ¬ ì˜ìƒ(ìŒì„±+í™”ë©´) ìë™ ìš”ì•½ê¸° â€” í™•ì¥í˜•")

with st.sidebar:
    st.header("ì„¤ì •")
    frame_interval_seconds = st.number_input("OCR í”„ë ˆì„ ê°„ê²©(ì´ˆ)", min_value=1, max_value=10, value=2)
    ocr_max_frames = st.number_input("OCR ìµœëŒ€ í”„ë ˆì„ ìˆ˜", min_value=10, max_value=1000, value=200)
    audio_chunk_minutes = st.number_input("ì˜¤ë””ì˜¤ ë¶„í•  ê¸¸ì´(ë¶„)", min_value=1, max_value=10, value=3)
    model_choice = st.selectbox("ìš”ì•½ì— ì‚¬ìš©í•  ëª¨ë¸", options=["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"], index=0)

st.markdown("### 1) ë¶„ì„í•  ì›¹í˜ì´ì§€ URL ì…ë ¥")
url = st.text_input("ì›¹í˜ì´ì§€ URL (YouTube ì˜ìƒ ë§í¬ê°€ í¬í•¨ëœ í˜ì´ì§€)")

if st.button("ë¶„ì„ ì‹œì‘") and url:
    tmpdir = Path(tempfile.mkdtemp(prefix="video_summarize_"))
    st.info("ğŸ” í˜ì´ì§€ì—ì„œ ì˜ìƒ ë§í¬ ì¶”ì¶œ ì¤‘...")
    links = extract_video_links_from_page(url)
    if not links:
        st.error("í•´ë‹¹ í˜ì´ì§€ì—ì„œ ìœ íŠœë¸Œ ì˜ìƒ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.success(f"ğŸ”— ì°¾ì€ ì˜ìƒ: {len(links)}ê°œ (ì²« ë²ˆì§¸ ì˜ìƒ ì²˜ë¦¬)")
        video_link = links[0]
        st.write(video_link)

        try:
            status_text = st.empty()
            status_text.info("ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            video_path = download_youtube_video(video_link, tmpdir)
            status_text.success("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

            status_text.info("ğŸ”Š ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘ (ffmpeg)...")
            audio_path = tmpdir / "extracted_audio.mp3"
            extract_audio_with_ffmpeg(video_path, audio_path)
            status_text.success("âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ")

            status_text.info("âœ‚ï¸ ì˜¤ë””ì˜¤ ë¶„í•  ì¤‘...")
            chunk_paths = split_audio_to_chunks(audio_path, chunk_minutes=int(audio_chunk_minutes))
            status_text.success(f"âœ… ì˜¤ë””ì˜¤ ë¶„í•  ì™„ë£Œ ({len(chunk_paths)}ê°œ ì²­í¬)")

            # ì˜¤ë””ì˜¤ ì „ì‚¬ (Whisper)
            progress_bar = st.progress(0)
            progress_msg = st.empty()
            def progress_cb(i, n, msg):
                progress_bar.progress(int((i+1)/n*100))
                progress_msg.text(msg)

            status_text.info("ğŸ¤ ìŒì„± ì „ì‚¬(Whisper) ì¤‘...")
            transcript_text = transcribe_audio_chunks(chunk_paths, progress_callback=progress_cb)
            status_text.success("âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ")
            progress_bar.empty()
            progress_msg.empty()

            # OCR
            status_text.info("ğŸ” í™”ë©´ OCR ì²˜ë¦¬ ì¤‘...")
            ocr_text = ocr_extract_text_from_video(video_path, frame_interval_seconds=int(frame_interval_seconds),
                                                   lang="kor+eng", max_frames=int(ocr_max_frames))
            status_text.success("âœ… í™”ë©´ OCR ì™„ë£Œ")

            # ìš”ì•½
            status_text.info("ğŸ§  GPTë¡œ ìš”ì•½ ìƒì„± ì¤‘...")
            combined_summary = summarize_combined_text(transcript_text, ocr_text, model=model_choice)
            status_text.success("âœ… ìš”ì•½ ì™„ë£Œ")

            st.markdown("## ê²°ê³¼")
            with st.expander("ì›ë¬¸ (ì˜¤ë””ì˜¤ ì „ì‚¬)"):
                st.write(transcript_text[:20000] + ("..." if len(transcript_text) > 20000 else ""))
            with st.expander("ì›ë¬¸ (OCR)"):
                st.write(ocr_text[:10000] + ("..." if len(ocr_text) > 10000 else ""))
            with st.expander("ìš”ì•½"):
                st.markdown(combined_summary)

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì œê³µ
            out_txt = tmpdir / "summary.txt"
            out_txt.write_text("SUMMARY\n\n" + combined_summary + "\n\n---\nAUDIO_TRANSCRIPT\n\n" + transcript_text + "\n\nOCR\n\n" + ocr_text, encoding="utf-8")
            with open(out_txt, "rb") as f:
                st.download_button("ìš”ì•½ + ì›ë¬¸ ë‹¤ìš´ë¡œë“œ (.txt)", f, file_name="video_summary.txt")
        except subprocess.CalledProcessError as e:
            st.error(f"ffmpeg ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
